#!/usr/bin/env python3
"""
sustainability_updates.py

Flow:
  1) fetch RSS -> raw_articles.json
  2) extract full text -> filtered_articles.json
  3) cluster -> clustered_articles.json
  4) summarize clusters -> summarized_articles.json

Notes:
 - Designed to run on CPU (GitHub Actions). Avoids launching Gradio when CI env var is set.
 - Be conservative with summarizer input length (truncate/join safely).
"""

import os
import json
from datetime import datetime, timedelta
from typing import List, Dict

import feedparser
import newspaper
import pandas as pd
from tqdm import tqdm

# NLP imports (heavy)
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from transformers import pipeline

# ----------------------------
# CONFIG
# ----------------------------
RAW_JSON = "raw_articles.json"
FILTERED_JSON = "filtered_articles.json"
CLUSTERED_JSON = "clustered_articles.json"
SUMMARIES_JSON = "summarized_articles.json"

NUM_CLUSTERS = 5
RSS_FEEDS = [
    # Current sources (keep these)
    "https://news.un.org/feed/subscribe/en/news/topic/climate-change/feed/rss.xml",
    "https://www.reuters.com/feeds/rss/environment",
    "https://www.theguardian.com/environment/rss",
    "https://www.cnbc.com/id/19836768/device/rss/rss.html",
    "https://www.euractiv.com/section/energy-environment/feed/",
    
    # NEW: European perspective
    "https://www.euronews.com/green/feed",
    "https://www.dw.com/rss/en_science-16726",
    
    # NEW: Middle Eastern/Global South perspective  
    "https://www.aljazeera.com/xml/rss/all.xml",  # You'll filter for environment topics in text extraction
    
    # NEW: Asian perspective
    "https://www.thehindu.com/sci-tech/energy-and-environment/feeder/default.rss",
    
    # NEW: Additional specialized sources
    "https://www.greenbiz.com/rss",  # Sustainable business
    "https://ens-newswire.com/feed/",  # Environment News Service
    "https://www.worldbank.org/en/topic/climatechange/rss.xml",
    "https://feeds.feedburner.com/EdieNews",  # Sustainability business news
    "https://www.climatechangenews.com/feed/",
    
    # NEW: US-focused but diverse
    "https://grist.org/feed/",  # Environmental journalism
    "https://insideclimatenews.org/feed/",
    
    # NEW: Ocean and biodiversity focus
    "https://news.mongabay.com/feed/",
]

# If running in CI (GitHub Actions sets CI=true), skip launching Gradio later.
IN_CI = os.environ.get("CI", "").lower() in ("1", "true", "yes")

# ----------------------------
# UTIL
# ----------------------------
def save_json(path: str, obj):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def load_json(path: str):
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# ----------------------------
# STEP 1: FETCH ARTICLES FROM RSS FEEDS (last 7 days)
# ----------------------------
def fetch_articles(feeds: List[str]) -> List[Dict]:
    articles = []
    now = datetime.utcnow()
    cutoff = now - timedelta(days=7)
    for url in feeds:
        try:
            feed = feedparser.parse(url)
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to parse feed {url}: {e}")
            continue

        for entry in feed.entries:
            # published may be missing or in different names; try a few fallbacks
            published_parsed = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
            if not published_parsed:
                # skip items without a parseable date
                continue
            pub_date = datetime(*published_parsed[:6])
            if pub_date < cutoff:
                continue

            title = getattr(entry, "title", None) or getattr(entry, "summary", "")[:140]
            link = getattr(entry, "link", None)
            source = None
            if hasattr(entry, "source"):
                # entry.source may be a dict-like
                try:
                    source = entry.source.get("title") if isinstance(entry.source, dict) else entry.source
                except Exception:
                    source = None

            articles.append({
                "title": title,
                "link": link,
                "published": pub_date.strftime("%Y-%m-%d"),
                "source": source or ""
            })
    return articles

# Run Step 1
articles = fetch_articles(RSS_FEEDS)
save_json(RAW_JSON, articles)
print(f"‚úÖ Step 1 complete: {len(articles)} articles saved to {RAW_JSON}")

# ----------------------------
# STEP 2: EXTRACT FULL TEXT, SOURCE, AND IMAGE FROM ARTICLES
# ----------------------------
import newspaper
import json

def extract_full_text(article_list):
    sustainability_keywords = [
        'sustainability', 'sustainable', 'climate', 'environment', 'green', 
        'renewable', 'carbon', 'emissions', 'eco', 'energy', 'conservation',
        'biodiversity', 'recycling', 'pollution', 'global warming', 'clean energy',
        'electric vehicle', 'solar', 'wind power', 'deforestation', 'organic',
        'circular economy', 'net zero', 'ESG', 'climate change', 'environmental',
        'cop28', 'cop29', 'global warming', 'greenhouse', 'zero emission'
    ]
    
    extracted = []
    for art in tqdm(article_list, desc="Extracting article text"):
        try:
            # Skip if title doesn't suggest sustainability content (for broad feeds)
            title = art.get("title", "").lower()
            if any(keyword in title for keyword in ['sports', 'entertainment', 'celebrity']):
                continue
                
            # Initialize newspaper article
            a = newspaper.Article(art["link"])
            a.download()
            a.parse()

            # Get source: either from RSS feed, newspaper detected, or fallback
            source_name = art.get("source") or getattr(a, "source_url", None) or "Unknown source"

            # Get top image (empty string if not found)
            top_image = getattr(a, "top_image", "") or ""

            article_text = getattr(a, "text", "")
            
            # Check if article is actually about sustainability (for broad feeds like Al Jazeera)
            combined_text = (title + " " + article_text).lower()
            if not any(keyword in combined_text for keyword in sustainability_keywords):
                continue  # Skip non-sustainability articles

            # Append extracted article info
            extracted.append({
                "title": art.get("title", "Untitled"),
                "link": art.get("link", "#"),
                "published": art.get("published", ""),
                "text": article_text,
                "source": source_name,
                "image": top_image
            })

        except Exception as e:
            # Skip articles that fail but continue processing
            print(f"‚ö†Ô∏è Skipped: {art.get('link', 'Unknown')} ({e})")

    return extracted

# Load raw articles saved in Step 1
RAW_JSON = "raw_articles.json"
FILTERED_JSON = "filtered_articles.json"

with open(RAW_JSON, "r", encoding="utf-8") as f:
    raw_articles = json.load(f)

# Extract full text, source, and image
filtered_articles = extract_full_text(raw_articles)

# Save to filtered JSON for Step 3
with open(FILTERED_JSON, "w", encoding="utf-8") as f:
    json.dump(filtered_articles, f, indent=2, ensure_ascii=False)

print(f"‚úÖ Step 2 complete: {len(filtered_articles)} articles saved to {FILTERED_JSON}")

# ----------------------------
# STEP 3: CLUSTER ARTICLES BY SIMILARITY
# ----------------------------
df = pd.DataFrame(filtered_articles)

if not df.empty:
    # Use a compact sentence-transformer (all-MiniLM...) which is CPU-friendly-ish
    print("üß† Loading sentence-transformers model for embeddings...")
    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = model.encode(df["text"].tolist(), show_progress_bar=True)

    n_clusters = min(NUM_CLUSTERS, len(df))
    if n_clusters < 1:
        n_clusters = 1

    print(f"üîÄ Running KMeans with n_clusters={n_clusters}")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    df["cluster"] = kmeans.fit_predict(embeddings)

    # drop heavy fields if necessary when saving clustered json (keep text truncated)
    clustered_for_save = df.copy()
    clustered_for_save["text"] = clustered_for_save["text"].str.slice(0, 6000)  # keep a chunk
    clustered_for_save = clustered_for_save.to_dict(orient="records")
    save_json(CLUSTERED_JSON, clustered_for_save)
    print(f"‚úÖ Step 3 complete: clusters saved to {CLUSTERED_JSON}")
else:
    print("‚ö†Ô∏è No articles to cluster; skipping Step 3.")
    save_json(CLUSTERED_JSON, [])
    df = pd.DataFrame()

# ----------------------------
# STEP 4: SUMMARIZE CLUSTERS (CPU-friendly)
# ----------------------------
clustered_data = load_json(CLUSTERED_JSON)
if not clustered_data:
    print("‚ö†Ô∏è No clustered data found. Skipping summarization.")
    save_json(SUMMARIES_JSON, [])
else:
    print("‚úÇÔ∏è Initializing summarizer (CPU)...")
    summarizer = pipeline(
        "summarization",
        model="sshleifer/distilbart-cnn-12-6",
        device=-1
    )

    df_clustered = pd.DataFrame(clustered_data)
    summaries = []

    for cluster_id in sorted(df_clustered["cluster"].unique()):
        subset = df_clustered[df_clustered["cluster"] == cluster_id]

        # Build joined input: titles + short text snippets (avoid sending whole articles)
        parts = []
        for _, row in subset.iterrows():
            title = row.get("title", "")[:300]
            src = row.get("source", "") or ""
            snippet = (row.get("text", "") or "")[:1200]  # keep snippet length sane
            parts.append(f"{title} ({src})\n{snippet}")

        joined_text = "\n\n".join(parts).strip()
        if not joined_text:
            joined_text = "No textual content available for these articles."

        # Truncate overall input to avoid token explosion (transformers CPU can choke)
        MAX_CHARS = 8000
        if len(joined_text) > MAX_CHARS:
            joined_text = joined_text[:MAX_CHARS]  # crude truncation; preserves start

        # Set summarizer length conservatively
        input_words = len(joined_text.split())
        max_len = max(60, min(200, input_words // 4))
        min_len = max(30, max_len // 3)

        try:
            result = summarizer(
                joined_text,
                max_length=max_len,
                min_length=min_len,
                do_sample=False
            )
            summary_text = result[0].get("summary_text", "").strip()
        except Exception as e:
            summary_text = f"‚ö†Ô∏è Summary failed: {e}"

        summaries.append({
            "cluster_id": int(cluster_id),
            "summary": summary_text,
            "sources": list(subset["source"].unique()),
            "articles": subset[["title", "link", "published", "source"]].to_dict(orient="records")
        })

    save_json(SUMMARIES_JSON, summaries)
    print(f"‚úÖ Step 4 complete: summaries saved to {SUMMARIES_JSON}")

# ----------------------------
# STEP 5: (optional) - Gradio viewer only when not in CI
# ----------------------------
def maybe_launch_gradio():
    try:
        import gradio as gr
    except Exception:
        print("‚ÑπÔ∏è Gradio not installed - skipping local viewer.")
        return

    if IN_CI:
        print("‚ÑπÔ∏è Running in CI - skipping Gradio server launch.")
        return

    # Only launch if explicitly requested via env var
    if os.environ.get("LAUNCH_GRADIO", "0") != "1":
        print("‚ÑπÔ∏è LAUNCH_GRADIO not set - skipping Gradio.")
        return

    def view_summaries():
        try:
            with open(SUMMARIES_JSON, "r", encoding="utf-8") as f:
                summaries = json.load(f)
        except Exception as e:
            return f"‚ö†Ô∏è Error loading summaries: {e}"

        out = ""
        for s in summaries:
            out += f"### Cluster {s['cluster_id']}\n\n"
            out += f"**Summary:** {s['summary']}\n\n"
            out += "**Articles:**\n"
            for a in s["articles"]:
                out += f"- [{a['title']}]({a['link']}) ({a.get('published','')})\n"
            out += "\n---\n"
        return out

    iface = gr.Interface(fn=view_summaries, inputs=None, outputs="markdown", title="Sustainability News")
    iface.launch(server_name="0.0.0.0", share=False)

if __name__ == "__main__":
    maybe_launch_gradio()
